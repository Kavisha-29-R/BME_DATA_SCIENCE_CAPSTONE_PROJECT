{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09ec6e1",
   "metadata": {},
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d98eb",
   "metadata": {},
   "source": [
    "This notebook explores the dataset:\n",
    "\n",
    "Regulation of Brain Cognitive States through Auditory, Gustatory, and Olfactory Stimulation with Wearable Monitoring (v1.0.0)\n",
    "from PhysioNet. https://physionet.org/content/brain-wearable-monitoring/1.0.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea01fd",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "This dataset encompasses the outcomes of two distinct experiments, with each containing participants' correct/incorrect responses, accompanied by their response times, documented as 'n_back_responses'. Alongside these behavioral responses, the dataset incorporates physiological measurements collected from two Empatica devices, each affixed to either hand, as well as data obtained from a muse headband. \n",
    "\n",
    "To present the data in a structured manner, each participant's data is contained within a designated folder, numbered sequentially from A1 to A10 in Experiment 1, and B1 to B10 in Experiment 2. The dataset also includes the data from participants who were excluded for further analysis in [1]. Their folder names are named 'Excluded_ID_number' [1]. Their ID numbers are according to those presented in supplementary materials in [1]. The column “Start_time_unix” shows the initial time of the recording expressed as UNIX timestamp in UTC.  \n",
    "\n",
    "The experiment settings and behavioral signals can be found in the ‘n_back_responses.csv’ file. The .csv file is in the following format:\n",
    "\n",
    "The “ExperimentName” specifies the experiment of the study.\n",
    "The “Instruction” specifies the presented n-back task at each trial.\n",
    "The “Running [Block]” displays the applied actuator within each session.\n",
    "In the designed n-back experiments, subjects were shown trials of stimulus (500 ms) along with a plus sign (fixation cross) for their response (1500 ms). The behavioral signals including the reaction time (RT) and correct/incorrect response are stored under the “Stimulus” and “Fixation” categories.\n",
    "\n",
    "In experiment 1, the reaction times (milliseconds) associated with session 1 to session 4 are stored within “Fixation101.RT” to “Fixation104.RT” and “Stimulus101.RT” to “Stimulus104.RT”. The binary correct/incorrect responses associated with session 1 to session 4 are stored within “Fixation101.ACC” to “Fixation104.ACC” and “Stimulus101.ACC” to “Stimulus104.ACC”.\n",
    "\n",
    "In experiment 2, the reaction times (milliseconds) associated with session 1 to session 3 are stored within “Fixation101.RT” to “Fixation103.RT” and “Stimulus101.RT” to “Stimulus103.RT”. The binary correct/incorrect responses associated with session 1 to session 3 are stored within “Fixation101.ACC” to “Fixation103.ACC” and “Stimulus101.ACC” to “Stimulus103.ACC”.\n",
    "\n",
    "The EEG signals recorded from the Muse headband are also included in the dataset as 'EEG_recording.csv'. They include EEG channels from four locations: TP9, AF7, AF8, and TP10. The 2016 model Muse (MU-02) can also output RAW EEG data at 256Hz as well as being able to output RAW EEG from the right ear USB Auxiliary connector (named Right AUX in the csv files). The column (timestamps) shows the data points in UNIX format. EEG measurements are labeled as TP9, AF7, AF8, TP10, Right AUX.\n",
    "\n",
    "The Empatica data encompasses a range of physiological aspects, including participants' electrodermal activity (EDA), heart rate (HR), blood volume pulses (BVP), skin surface temperature, Photoplethysmography (PPG), and 3-axis accelerometer data. These data types are stored as separate CSV files: 'EDA.csv', 'TEMP.csv', 'ACC.csv', 'BVP.csv', 'HR.csv', 'IBI.csv', and 'tags.csv'. Data recorded by Empatica wristbands from Left and Right hands are each represented by their own distinct CSV files, encompassing the following attributes:\n",
    "\n",
    "- 'Left_EDA.csv' and 'Right_EDA.csv'\n",
    "- 'Left_TEMP.csv' and 'Right_TEMP.csv'\n",
    "- 'Left_ACC.csv' and 'Right_ACC.csv'\n",
    "- 'Left_BVP.csv' and 'Right_BVP.csv'\n",
    "- 'Left_HR.csv' and 'Right_HR.csv'\n",
    "- 'Left_IBI.csv' and 'Right_IBI.csv'\n",
    "- 'Left_tags.csv' and 'Right_tags.csv'\n",
    "\n",
    "For clarity, a concise summary of the contents of the different data files is provided below:\n",
    "\n",
    "- EDA: Measurements from the electrodermal activity (EDA) sensor expressed as micro siemens (μS). Values in the first column (EDA) show the EDA values. The column (start_time_unix) shows the initial time of the recording expressed as UNIX timestamp in UTC. The column (sampling_rate) shows the sample rate expressed in Hz.\n",
    "- TEMP: Skin temperature (TEMP) measured from temperature sensor expressed degrees on the Celsius (°C) scale. Values in the first column (TEMP) show the TEMP values. The column (start_time_unix) shows the initial time of the recording expressed as UNIX timestamp in UTC. The column (sampling_rate) shows the sample rate expressed in Hz.\n",
    "- ACC: Data from 3-axis accelerometer sensor. The accelerometer is configured to measure acceleration in the range [-2g, 2g]. Therefore, the unit in this file is 1/64g. Data from x, y, and z axis are labeled respectively (i.e., ACC_X, ACC_Y, and ACC_Z).  The column (start_time_unix) shows the initial time of the recording expressed as UNIX timestamp in UTC. The column (sampling_rate) shows the sample rate expressed in Hz.\n",
    "- BVP: Blood volume pulses (BVP) measured from a photoplethysmograph. Values in the first column (BVP) show the BVP values. The column (start_time_unix) shows the initial time of the recording expressed as UNIX timestamp in UTC. The column (sampling_rate) shows the sample rate expressed in Hz.\n",
    "- HR: Average heart rate (HR) extracted from the BVP signal. Values in the first column (HR) show the HR values. The column (start_time_unix) shows the initial time of the recording expressed as UNIX timestamp in UTC. The column (sampling_rate) shows the sample rate expressed in Hz.\n",
    "- IBI: Time between individuals heart beats extracted from the BVP signal. The first column (IBI_time) shows the time (with respect to the initial time) of the detected inter-beat interval expressed in seconds (s). The second column (IBI_intervals) shows the duration in seconds (s) of the detected inter-beat interval (i.e., the distance in seconds from the previous beat). No sample rate is needed for this file.\n",
    "- tags: Event mark times in each Empatica device. Each row corresponds to a physical button press on the device; at the same time as the status LED is first illuminated. The time is expressed as a UNIX timestamp in UTC, and it is synchronized with the initial time of the recordings indicated in the related data files from the corresponding session.\n",
    "Due to the inadvertent receipt of certain tags during experiments, the correct and unified tags are also included as 'tags.csv'.\n",
    "\n",
    "[1] Fekri Azgomi, Hamid, et al. \"Regulation of brain cognitive states through auditory, gustatory, and olfactory stimulation with wearable monitoring.\" Scientific Reports 13.1 (2023): 12399. https://www.nature.com/articles/s41598-023-37829-z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e9702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.signal import welch, find_peaks\n",
    "from scipy.stats import iqr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46fef82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumptions (as specified in data description):\n",
    "# - Each participant folder (A1..A10, B1..B10, Excluded_...) contains:\n",
    "#     n_back_responses.csv\n",
    "#     EEG_recordings.csv\n",
    "#     Left_ACC.csv, Right_ACC.csv, Left_BVP.csv, Right_BVP.csv, Left_EDA.csv, Right_EDA.csv, Left_HR.csv, Right_HR.csv,\n",
    "#     Left_IBI.csv, Right_IBI.csv, Left_TEMP.csv, Right_TEMP.csv, tags.csv (or Left_tags/Right_tags)\n",
    "# - Empatica CSVs have columns (ACC_X,ACC_Y,ACC_Z,start_time_unix,sampling_rate, etc.)\n",
    "# - Muse EEG CSV has 'timestamps' (UNIX seconds or ms) and channels TP9, AF7, AF8, TP10, Right AUX\n",
    "# - Trial structure: trials spaced every 2.0 s per session block (500 ms stimulus + 1500 ms fixation)\n",
    "# - Reaction times in Stimulus###.RT / Fixation###.RT are in milliseconds (ms) and ACC columns indicate correctness (1==correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bc0f03",
   "metadata": {},
   "source": [
    "Data stored locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f9532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- dataset folder ----------\n",
    "BASE_PATH = Path(r\"C:\\Users\\Kavisha\\OneDrive - Johns Hopkins\\EN.585.771_BME_DATASCIENCE\\Capstone_Project\\App\\BME_DATA_SCIENCE_CAPSTONE_PROJECT\\Data\\Brain_Cognitive_States_Data\")\n",
    "EXP1 = BASE_PATH / \"Experiment_1\"\n",
    "EXP2 = BASE_PATH / \"Experiment_2\"\n",
    "OUT_DIR = BASE_PATH / \"processed_features_v2\"\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a8104",
   "metadata": {},
   "source": [
    "Functions for handling files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88b6e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eeg(eeg_path):\n",
    "    df = pd.read_csv(eeg_path)\n",
    "    tcol = next((c for c in df.columns if 'time' in c.lower() or 'timestamp' in c.lower()), None)\n",
    "    if tcol is None:\n",
    "        raise ValueError(\"EEG file missing timestamps\")\n",
    "    \n",
    "    if df[tcol].median() > 1e9:  # ms in UNIX ms\n",
    "        df['time_s'] = df[tcol] / 1000.0\n",
    "    else:\n",
    "        df['time_s'] = df[tcol]\n",
    "    df = df.set_index('time_s').sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_empatica_file(path, kind):\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    if 'start_time_unix' not in [c.lower() for c in df.columns]:\n",
    "        raise ValueError(f\"{path.name} missing start_time_unix column\")\n",
    "    # ensure column names as original (case-sensitive) accessible\n",
    "    # find the actual column names for start_time_unix and sampling_rate if they have different case\n",
    "    start_col = next(c for c in df.columns if c.lower() == 'start_time_unix')\n",
    "    sr_col = next((c for c in df.columns if c.lower() == 'sampling_rate'), None)\n",
    "    st = float(df[start_col].dropna().iloc[0])\n",
    "    sr = float(df[sr_col].dropna().iloc[0]) if sr_col else None\n",
    "\n",
    "    if kind.lower() == 'acc':\n",
    "        # expect ACC_X,ACC_Y,ACC_Z present\n",
    "        # create time axis from index and sampling rate\n",
    "        if sr is None:\n",
    "            raise ValueError(f\"ACC file {path} missing sampling_rate\")\n",
    "        n = len(df)\n",
    "        times = st + (np.arange(n) / sr)\n",
    "        out = df.copy().reset_index(drop=True)\n",
    "        out['time_s'] = times\n",
    "        out = out.set_index('time_s').sort_index()\n",
    "        # keep ACC_X/Y/Z\n",
    "        cols = [c for c in out.columns if c.lower() in ('acc_x','acc_y','acc_z')]\n",
    "        return out[cols].astype(float)\n",
    "    elif kind.lower() in ('bvp','eda','hr','temp'):\n",
    "        if sr is None:\n",
    "            raise ValueError(f\"{path} missing sampling_rate\")\n",
    "        n = len(df)\n",
    "        times = st + (np.arange(n) / sr)\n",
    "        # pick the measurement column (first column that's not start_time_unix or sampling_rate)\n",
    "        meas_col = next((c for c in df.columns if c.lower() not in ('start_time_unix','sampling_rate')), df.columns[0])\n",
    "        out = pd.DataFrame({meas_col: df[meas_col].values}, index=times)\n",
    "        out.index.name = 'time_s'\n",
    "        return out.astype(float)\n",
    "    elif kind.lower() == 'ibi':\n",
    "        # expects IBI_time (s) and IBI_intervals (s)\n",
    "        # absolute times = start_time_unix + IBI_time\n",
    "        ibi_time_col = next((c for c in df.columns if c.lower() == 'ibi_time'), None)\n",
    "        ibi_int_col = next((c for c in df.columns if 'ibi_intervals' in c.lower() or 'ibi_interval' in c.lower()), None)\n",
    "        if ibi_time_col is None:\n",
    "            # try lowercase names\n",
    "            raise ValueError(f\"IBI file {path} missing IBI_time column\")\n",
    "        abs_times = st + df[ibi_time_col].astype(float).values\n",
    "        out = pd.DataFrame({'IBI_interval': df[ibi_int_col].astype(float).values}, index=abs_times)\n",
    "        out.index.name = 'time_s'\n",
    "        return out\n",
    "    else:\n",
    "        raise ValueError(\"Unknown empatica kind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed2954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Behavior parsing (uses the actual Stimulus###.RT & Fixation###.RT style columns) ----------\n",
    "def load_behavior(participant_folder):\n",
    "    bf = participant_folder / 'n_back_responses.csv'\n",
    "    if not bf.exists():\n",
    "        raise FileNotFoundError(f\"Behavior file missing for {participant_folder}\")\n",
    "    df = pd.read_csv(bf)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a74487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_tags(participant_folder):\n",
    "    # prefer unified tags.csv, else Left_tags or Right_tags, else None\n",
    "    for fname in ['tags.csv','Tags.csv','left_tags.csv','Left_tags.csv','right_tags.csv','Right_tags.csv']:\n",
    "        p = participant_folder / fname\n",
    "        if p.exists():\n",
    "            arr = pd.read_csv(p, header=None).iloc[:,0].astype(float).tolist()\n",
    "            return arr\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ccddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_behavior_events(behavior_df, tags_list, experiment_label):\n",
    "    \"\"\"\n",
    "    Convert stimulus/fixation RT & ACC columns into event rows with absolute timestamps (seconds).\n",
    "    - behavior_df: per-participant n_back_responses.csv\n",
    "    - tags_list: list of session start UNIX times in seconds (tags.csv)\n",
    "    - experiment_label: 'Experiment_1' or 'Experiment_2'\n",
    "    \n",
    "    Returns DataFrame with columns:\n",
    "    event_time_s, trial_type, session_idx, trial_idx, RT_s, ACC\n",
    "    \"\"\"\n",
    "    # ---------------------------\n",
    "    # 1. Identify session type column\n",
    "    # ---------------------------\n",
    "    if \"Running [Block]\" in behavior_df.columns:\n",
    "        block_col = \"Running [Block]\"\n",
    "    elif \"Running[Block]\" in behavior_df.columns:\n",
    "        block_col = \"Running[Block]\"\n",
    "    else:\n",
    "        block_col = None\n",
    "\n",
    "    if behavior_df is None or behavior_df.shape[0] == 0:\n",
    "        return pd.DataFrame(columns=[\n",
    "            'event_time_s', 'trial_type', 'session_idx', 'trial_idx',\n",
    "            'RT_s', 'ACC', 'session_type'\n",
    "        ])\n",
    "        \n",
    "    # ---------------------------\n",
    "    # 2. RT and ACC column detection\n",
    "    # ---------------------------\n",
    "    stim_rt_cols = sorted([c for c in behavior_df.columns \n",
    "                           if re.match(r'(?i)stimulus[0-9]{3}\\.RT$', c) or re.match(r'(?i)stimulus[0-9]{3}_RT$', c)])\n",
    "\n",
    "    stim_acc_cols = sorted([c for c in behavior_df.columns \n",
    "                            if re.match(r'(?i)stimulus[0-9]{3}\\.ACC$', c) or re.match(r'(?i)stimulus[0-9]{3}_ACC$', c)])\n",
    "\n",
    "    fix_rt_cols = sorted([c for c in behavior_df.columns \n",
    "                          if re.match(r'(?i)fixation[0-9]{3}\\.RT$', c) or re.match(r'(?i)fixation[0-9]{3}_RT$', c)])\n",
    "\n",
    "    fix_acc_cols = sorted([c for c in behavior_df.columns \n",
    "                           if re.match(r'(?i)fixation[0-9]{3}\\.ACC$', c) or re.match(r'(?i)fixation[0-9]{3}_ACC$', c)])\n",
    "       \n",
    "    # ---------------------------\n",
    "    # 3. Build maps for each session code\n",
    "    # ---------------------------\n",
    "    stim_map = {}\n",
    "    for c in stim_rt_cols:\n",
    "        code = int(re.search(r'([0-9]{3})', c).group(1))\n",
    "        stim_map.setdefault(code, {})['rt'] = behavior_df[c].values\n",
    "\n",
    "    for c in stim_acc_cols:\n",
    "        code = int(re.search(r'([0-9]{3})', c).group(1))\n",
    "        stim_map.setdefault(code, {})['acc'] = behavior_df[c].values\n",
    "\n",
    "    fix_map = {}\n",
    "    for c in fix_rt_cols:\n",
    "        code = int(re.search(r'([0-9]{3})', c).group(1))\n",
    "        fix_map.setdefault(code, {})['rt'] = behavior_df[c].values\n",
    "\n",
    "    for c in fix_acc_cols:\n",
    "        code = int(re.search(r'([0-9]{3})', c).group(1))\n",
    "        fix_map.setdefault(code, {})['acc'] = behavior_df[c].values\n",
    "        \n",
    "    # ---------------------------\n",
    "    # 4. Expected sessions (Exp1 = 4, Exp2 = 3)\n",
    "    # ---------------------------\n",
    "    if '1' in str(experiment_label):\n",
    "        expected_codes = [101, 102, 103, 104]\n",
    "    else:\n",
    "        expected_codes = [101, 102, 103]\n",
    "        \n",
    "    # ---------------------------\n",
    "    # 5. Session start timestamps\n",
    "    # ---------------------------\n",
    "    if len(tags_list) == 0:\n",
    "        st_col = next((c for c in behavior_df.columns if c.lower() == 'start_time_unix'), None)\n",
    "        if st_col:\n",
    "            base_time = float(behavior_df[st_col].dropna().iloc[0])\n",
    "            tags_list = [base_time + i*1.0 for i in range(len(expected_codes))]\n",
    "        else:\n",
    "            tags_list = [0.0] * len(expected_codes)\n",
    "            \n",
    "    # ---------------------------\n",
    "    # 6. Build event rows\n",
    "    # ---------------------------\n",
    "    events = []\n",
    "    for sidx, code in enumerate(expected_codes):\n",
    "\n",
    "        session_start = tags_list[sidx] if sidx < len(tags_list) else tags_list[-1]\n",
    "\n",
    "        stim_entry = stim_map.get(code, {})\n",
    "        fix_entry = fix_map.get(code, {})\n",
    "\n",
    "        n_trials = len(stim_entry.get('rt', [])) if 'rt' in stim_entry \\\n",
    "                   else len(fix_entry.get('rt', [])) if 'rt' in fix_entry else 0\n",
    "\n",
    "        for j in range(n_trials):\n",
    "\n",
    "            # Extract session type (same row for stimulus & fixation)\n",
    "            session_type_val = None\n",
    "            if block_col is not None and j < len(behavior_df):\n",
    "                session_type_val = behavior_df[block_col].iloc[j]\n",
    "\n",
    "            # ------------------ Stimulus ------------------\n",
    "            stim_onset = session_start + j * 2.0\n",
    "\n",
    "            # RT\n",
    "            rt_s = None\n",
    "            if 'rt' in stim_entry and j < len(stim_entry['rt']):\n",
    "                try:\n",
    "                    rt_ms = float(stim_entry['rt'][j])\n",
    "                    rt_s = rt_ms / 1000.0 if (rt_ms > 0 and np.isfinite(rt_ms)) else None\n",
    "                except:\n",
    "                    rt_s = None\n",
    "\n",
    "            # ACC\n",
    "            acc_s = None\n",
    "            if 'acc' in stim_entry and j < len(stim_entry['acc']):\n",
    "                try:\n",
    "                    acc_s = int(stim_entry['acc'][j])\n",
    "                except:\n",
    "                    acc_s = None\n",
    "\n",
    "            events.append({\n",
    "                'event_time_s': stim_onset + rt_s if rt_s else stim_onset,\n",
    "                'trial_type': 'stimulus',\n",
    "                'session_idx': sidx+1,\n",
    "                'trial_idx': j+1,\n",
    "                'RT_s': rt_s if rt_s else np.nan,\n",
    "                'ACC': acc_s,\n",
    "                'session_type': session_type_val,\n",
    "            })\n",
    "\n",
    "            # ------------------ Fixation ------------------\n",
    "            fix_onset = stim_onset + 0.5\n",
    "\n",
    "            rt_f = None\n",
    "            if 'rt' in fix_entry and j < len(fix_entry['rt']):\n",
    "                try:\n",
    "                    rt_ms = float(fix_entry['rt'][j])\n",
    "                    rt_f = rt_ms / 1000.0 if (rt_ms > 0 and np.isfinite(rt_ms)) else None\n",
    "                except:\n",
    "                    rt_f = None\n",
    "\n",
    "            acc_f = None\n",
    "            if 'acc' in fix_entry and j < len(fix_entry['acc']):\n",
    "                try:\n",
    "                    acc_f = int(fix_entry['acc'][j])\n",
    "                except:\n",
    "                    acc_f = None\n",
    "\n",
    "            events.append({\n",
    "                'event_time_s': fix_onset + rt_f if rt_f else fix_onset,\n",
    "                'trial_type': 'fixation',\n",
    "                'session_idx': sidx+1,\n",
    "                'trial_idx': j+1,\n",
    "                'RT_s': rt_f if rt_f else np.nan,\n",
    "                'ACC': acc_f,\n",
    "                'session_type': session_type_val,\n",
    "            })\n",
    "            \n",
    "    # ---------------------------\n",
    "    # 7. Final dataframe\n",
    "    # ---------------------------\n",
    "    evdf = pd.DataFrame(events)\n",
    "    evdf = evdf.sort_values('event_time_s').reset_index(drop=True)\n",
    "\n",
    "    return evdf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4db045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Feature extraction ----------\n",
    "def bandpower(arr, fs, band):\n",
    "    if len(arr) < 4 or np.all(np.isnan(arr)):\n",
    "        return np.nan\n",
    "    f,Pxx = welch(arr, fs=fs, nperseg=min(256, len(arr)))\n",
    "    mask = (f>=band[0]) & (f<=band[1])\n",
    "    if mask.sum()==0:\n",
    "        return 0.0\n",
    "    return np.trapz(Pxx[mask], f[mask])\n",
    "\n",
    "BANDS = {'delta':(1,4),'theta':(4,8),'alpha':(8,13),'beta':(13,30),'gamma':(30,45)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_magnitude(acc_df):\n",
    "    arr = acc_df[['ACC_X','ACC_Y','ACC_Z']].astype(float).values\n",
    "    return np.linalg.norm(arr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ibi_features(ibi_df):\n",
    "    arr = ibi_df['IBI_interval'].dropna().values\n",
    "    if len(arr)==0:\n",
    "        return {'ibi_count':0,'ibi_sdnn':np.nan,'ibi_rmssd':np.nan}\n",
    "    sdnn = np.std(arr, ddof=1)\n",
    "    rmssd = np.sqrt(np.mean(np.diff(arr)**2)) if len(arr)>1 else np.nan\n",
    "    return {'ibi_count':len(arr),'ibi_sdnn':sdnn,'ibi_rmssd':rmssd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a543a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_window_features(common_time, resampled, fs=10., win=5.0, step=2.5):\n",
    "    rows=[]\n",
    "    starts = np.arange(common_time[0], common_time[-1]-win+1e-6, step)\n",
    "    for s in starts:\n",
    "        e = s+win\n",
    "        row = {'window_start':s,'window_end':e,'window_center':s+win/2}\n",
    "        # EEG bandpower per channel \n",
    "        if 'eeg' in resampled:\n",
    "            seg = resampled['eeg'].loc[(resampled['eeg'].index>=s)&(resampled['eeg'].index<e)]\n",
    "            if seg.shape[0]>2:\n",
    "                for ch in seg.columns:\n",
    "                    for bname, br in BANDS.items():\n",
    "                        row[f\"{ch}_{bname}\"] = bandpower(seg[ch].values, fs=fs, band=br)\n",
    "            else:\n",
    "                for ch in ['TP9','AF7','AF8','TP10','Right AUX']:\n",
    "                    for bname in BANDS:\n",
    "                        row[f\"{ch}_{bname}\"] = np.nan\n",
    "        # EDA\n",
    "        for side in ['left_eda','right_eda']:\n",
    "            if side in resampled:\n",
    "                seg = resampled[side].loc[(resampled[side].index>=s)&(resampled[side].index<e)]\n",
    "                if seg.shape[0]>0:\n",
    "                    col = seg.columns[0]\n",
    "                    vals = seg[col].astype(float).values\n",
    "                    row[f\"{side}_mean\"] = np.nanmean(vals)\n",
    "                    row[f\"{side}_std\"] = np.nanstd(vals)\n",
    "                    try:\n",
    "                        pks,_ = find_peaks(vals, height=np.nanmean(vals)+0.5*np.nanstd(vals))\n",
    "                        row[f\"{side}_n_peaks\"] = len(pks)\n",
    "                    except Exception:\n",
    "                        row[f\"{side}_n_peaks\"] = 0\n",
    "                else:\n",
    "                    row[f\"{side}_mean\"]=np.nan; row[f\"{side}_std\"]=np.nan; row[f\"{side}_n_peaks\"]=0\n",
    "        # BVP -> HR approx\n",
    "        for side in ['left_bvp','right_bvp']:\n",
    "            if side in resampled:\n",
    "                seg = resampled[side].loc[(resampled[side].index>=s)&(resampled[side].index<e)]\n",
    "                if seg.shape[0]>3:\n",
    "                    vals = seg.iloc[:,0].astype(float).values\n",
    "                    try:\n",
    "                        pks,_ = find_peaks(vals, distance=int(fs*0.4))\n",
    "                        if len(pks)>=2:\n",
    "                            idxs = seg.index.values\n",
    "                            mean_dt = np.nanmean(np.diff(idxs)) if len(idxs)>1 else 1.0/fs\n",
    "                            rr = np.diff(pks) * mean_dt\n",
    "                            hr = 60.0/rr\n",
    "                            row[f\"{side}_hr_mean\"] = np.nanmean(hr)\n",
    "                            row[f\"{side}_hr_std\"] = np.nanstd(hr)\n",
    "                            row[f\"{side}_n_beats\"] = len(pks)\n",
    "                        else:\n",
    "                            row[f\"{side}_hr_mean\"]=np.nan; row[f\"{side}_hr_std\"]=np.nan; row[f\"{side}_n_beats\"]=len(pks)\n",
    "                    except Exception:\n",
    "                        row[f\"{side}_hr_mean\"]=np.nan; row[f\"{side}_hr_std\"]=np.nan; row[f\"{side}_n_beats\"]=0\n",
    "                else:\n",
    "                    row[f\"{side}_hr_mean\"]=np.nan; row[f\"{side}_hr_std\"]=np.nan; row[f\"{side}_n_beats\"]=0\n",
    "        # ACC metrics\n",
    "        for side in ['left_acc','right_acc']:\n",
    "            if side in resampled:\n",
    "                seg = resampled[side].loc[(resampled[side].index>=s)&(resampled[side].index<e)]\n",
    "                if seg.shape[0]>0:\n",
    "                    mag = acc_magnitude(seg)\n",
    "                    row[f\"{side}_mag_mean\"] = np.nanmean(mag)\n",
    "                    row[f\"{side}_mag_std\"] = np.nanstd(mag)\n",
    "                    row[f\"{side}_mag_iqr\"] = iqr(mag) if len(mag)>0 else np.nan\n",
    "                else:\n",
    "                    row[f\"{side}_mag_mean\"]=np.nan; row[f\"{side}_mag_std\"]=np.nan; row[f\"{side}_mag_iqr\"]=np.nan\n",
    "        # TEMP\n",
    "        for side in ['left_temp','right_temp']:\n",
    "            if side in resampled:\n",
    "                seg = resampled[side].loc[(resampled[side].index>=s)&(resampled[side].index<e)]\n",
    "                if seg.shape[0]>0:\n",
    "                    col = seg.columns[0]\n",
    "                    row[f\"{side}_mean\"] = np.nanmean(seg[col].astype(float).values)\n",
    "                    row[f\"{side}_std\"] = np.nanstd(seg[col].astype(float).values)\n",
    "                else:\n",
    "                    row[f\"{side}_mean\"]=np.nan; row[f\"{side}_std\"]=np.nan\n",
    "        # IBI HRV\n",
    "        for side in ['left_ibi','right_ibi']:\n",
    "            if side in resampled:\n",
    "                ibi_seg = resampled[side].loc[(resampled[side].index>=s)&(resampled[side].index<e)]\n",
    "                if ibi_seg.shape[0]>0:\n",
    "                    h = ibi_features(ibi_seg)\n",
    "                    row.update({f\"{side}_{k}\":v for k,v in h.items()})\n",
    "                else:\n",
    "                    row[f\"{side}_ibi_count\"]=0; row[f\"{side}_ibi_sdnn\"]=np.nan; row[f\"{side}_ibi_rmssd\"]=np.nan\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Per-participant processing ----------\n",
    "def process_participant(participant_folder, common_fs=10.0, win_s=5.0, step_s=2.5):\n",
    "    print(\"Processing:\", participant_folder.name)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1. Load behavior and tags\n",
    "    # ---------------------------\n",
    "    behavior_df = load_behavior(participant_folder)\n",
    "    tags = get_session_tags(participant_folder)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2. Parse events (includes session_type from your updated function)\n",
    "    # ---------------------------\n",
    "    events = parse_behavior_events(\n",
    "        behavior_df, tags, participant_folder.parent.name\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3. Load signals (EEG + Empatica)\n",
    "    # ---------------------------\n",
    "    signals = {}\n",
    "\n",
    "    # EEG\n",
    "    eeg_p = participant_folder / 'EEG_recordings.csv'\n",
    "    if eeg_p.exists():\n",
    "        signals['eeg'] = load_eeg(eeg_p)\n",
    "\n",
    "    # Empatica left and right wrist data\n",
    "    for side in ['Left', 'Right']:\n",
    "        for kind in ['ACC', 'BVP', 'EDA', 'HR', 'IBI', 'TEMP']:\n",
    "            p = participant_folder / f\"{side}_{kind}.csv\"\n",
    "            if p.exists():\n",
    "                try:\n",
    "                    signals[f\"{side.lower()}_{kind.lower()}\"] = load_empatica_file(\n",
    "                        p, kind.lower()\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning loading {p.name}: {e}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4. Build common time axis\n",
    "    # ---------------------------\n",
    "    t_mins, t_maxs = [], []\n",
    "    for df in signals.values():\n",
    "        t_mins.append(df.index.min())\n",
    "        t_maxs.append(df.index.max())\n",
    "\n",
    "    if len(t_mins) == 0:\n",
    "        raise ValueError(f\"No signals found for {participant_folder.name}\")\n",
    "\n",
    "    overall_min = min([t for t in t_mins if pd.notna(t)])\n",
    "    overall_max = max([t for t in t_maxs if pd.notna(t)])\n",
    "\n",
    "    common_time = np.arange(overall_min, overall_max, 1.0 / common_fs)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 5. Resample all signals to common_time\n",
    "    # ---------------------------\n",
    "    resampled = {}\n",
    "    for k, df in signals.items():\n",
    "        try:\n",
    "            df2 = df.copy()\n",
    "            df2.index = df2.index.astype(float)\n",
    "            resampled[k] = df2.reindex(\n",
    "                common_time, method='nearest', tolerance=1.0 / common_fs\n",
    "            )\n",
    "        except Exception:\n",
    "            # fallback interpolation if needed\n",
    "            resampled[k] = df.reindex(common_time).interpolate().ffill().bfill()\n",
    "\n",
    "    # ---------------------------\n",
    "    # 6. Extract window-level features\n",
    "    # ---------------------------\n",
    "    feats = extract_window_features(\n",
    "        common_time, resampled, fs=common_fs, win=win_s, step=step_s\n",
    "    )\n",
    "    feats['participant'] = participant_folder.name\n",
    "\n",
    "    # Prepare behavioral summary per-window\n",
    "    feats['n_events_in_window'] = 0\n",
    "    feats['mean_RT'] = np.nan\n",
    "    feats['prop_correct'] = np.nan\n",
    "    feats['session_type'] = None  # <-- NEW COLUMN\n",
    "\n",
    "    # ---------------------------\n",
    "    # 7. Add behavior + session_type to windows\n",
    "    # ---------------------------\n",
    "    for idx, win in feats.iterrows():\n",
    "        s = win['window_start']\n",
    "        e = win['window_end']\n",
    "\n",
    "        evs = events[(events['event_time_s'] >= s) & (events['event_time_s'] < e)]\n",
    "        feats.at[idx, 'n_events_in_window'] = len(evs)\n",
    "\n",
    "        # Mean RT\n",
    "        if len(evs) > 0:\n",
    "            rts = evs['RT_s'].dropna().astype(float).values\n",
    "            feats.at[idx, 'mean_RT'] = np.nanmean(rts) if len(rts) > 0 else np.nan\n",
    "\n",
    "            accs = evs['ACC'].dropna().astype(float).values\n",
    "            feats.at[idx, 'prop_correct'] = (\n",
    "                np.nanmean((accs == 1).astype(float)) if len(accs) > 0 else np.nan\n",
    "            )\n",
    "\n",
    "            # ---- Assign session type for the window ----\n",
    "            if 'session_type' in evs.columns:\n",
    "                # pick the most common session type in that window\n",
    "                st_series = evs['session_type'].dropna()\n",
    "                if len(st_series) > 0:\n",
    "                    feats.at[idx, 'session_type'] = st_series.mode().iloc[0]\n",
    "                else:\n",
    "                    feats.at[idx, 'session_type'] = None\n",
    "            else:\n",
    "                feats.at[idx, 'session_type'] = None\n",
    "\n",
    "    return {\n",
    "        'participant': participant_folder.name,\n",
    "        'features': feats,\n",
    "        'events': events,\n",
    "        'resampled': resampled\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ---------- Process all participants in an experiment ----------\n",
    "def process_experiment(exp_folder, out_dir=OUT_DIR, participants=None):\n",
    "    results = {}\n",
    "    all_feats=[]\n",
    "    \n",
    "    for sub in sorted(exp_folder.iterdir()):\n",
    "        if not sub.is_dir(): continue\n",
    "        if participants and sub.name not in participants: continue\n",
    "        try:\n",
    "            res = process_participant(sub)\n",
    "            results[sub.name] = res\n",
    "            all_feats.append(res['features'])\n",
    "            # save per-subject\n",
    "            res['features'].to_parquet(Path(out_dir)/f\"{sub.name}_features.parquet\")\n",
    "            res['events'].to_csv(Path(out_dir)/f\"{sub.name}_events.csv\", index=False)\n",
    "            print(f\"Saved {sub.name} features/events\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed {sub.name}: {e}\")\n",
    "    if len(all_feats)==0:\n",
    "        return pd.DataFrame(), results\n",
    "    combined = pd.concat(all_feats, ignore_index=True)\n",
    "    combined.to_parquet(Path(out_dir)/f\"{exp_folder.name}_all_features.parquet\")\n",
    "    combined.to_csv(Path(out_dir)/f\"{exp_folder.name}_all_features.csv\", index=False)\n",
    "    return combined, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Run for both experiments ----------\n",
    "exp1_feats, exp1_res = process_experiment(EXP1)\n",
    "exp2_feats, exp2_res = process_experiment(EXP2)\n",
    "full_df = pd.concat([exp1_feats, exp2_feats], ignore_index=True)\n",
    "full_df.to_parquet(OUT_DIR/\"all_participants_features.parquet\")\n",
    "full_df.to_csv(OUT_DIR/\"all_participants_features.csv\", index=False)\n",
    "print(\"All done. Saved combined features to:\", OUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognitive_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
